{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Implementation of Llama 3.2 for GitHub Pull Request Comments\n",
    "\n",
    "This notebook outlines the process of implementing a basic prompt and response system using the Llama 3.2 model from Hugging Face. The primary goal is to generate a GitHub pull request comment that serves as documentation for a provided code snippet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting Up the Environment\n",
    "\n",
    "Before running `base_pretrained_model.ipynb`, ensure you have the necessary libraries installed. The key libraries used in this implementation are:\n",
    "* `torch`\n",
    "* `transformers`\n",
    "* `huggingface_hub`\n",
    "\n",
    "To get started, follow the instructions in the [Hugging Face quickstart guide](https://huggingface.co/docs/datasets/en/quickstart) and the [PyTorch installation guide](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "Additionally, you will need a Hugging Face access token, which can be found under your account settings in the Access Tokens tab. Create a token (note that some LLMs may require you to specify a repository during token creation) and store it securely. For this implementation, I store mine in a separate file, which is why I import hf_token alongside the other libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Libraries and Clone LLM Repository\n",
    "\n",
    "The code below imports the required libraries and logs in to Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "from hf_token import llama_3_2_token\n",
    "\n",
    "login(token=llama_3_2_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cloning The Repository**\n",
    "\n",
    "Make sure to clone the repository for the desired LLM, which in this case is `meta-llama/Llama-3.2-1B`. Ensure the repository is cloned into your working directory. You will need your access token and username during this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating the Test Prompt\n",
    "\n",
    "For this basic implementation, I will create a prompt that instructs the model to write a GitHub pull request comment for a provided code snippet. The code being tested is a safe division function.\n",
    "\n",
    "For now, I will store the prompt in a variable called `prompt`. In the future, it may be beneficial to create a function to generate prompts dynamically for enhanced usability.\n",
    "\n",
    "\n",
    "**Safe Division Function** \n",
    "\n",
    "Hereâ€™s the code for the safe division function:\n",
    "\n",
    "```python\n",
    "    from typing import Union\n",
    "    \n",
    "    def safe_divide(a: Union[int, float], b: Union[int, float]) -> Union[float, None]:\n",
    "        \"\"\"Safely divides two numbers, returning None if division by zero occurs.\"\"\"\n",
    "        try:\n",
    "            return a / b\n",
    "        except ZeroDivisionError:\n",
    "            print(\"Error: Division by zero is not allowed.\")\n",
    "            return None\n",
    "```\n",
    "\n",
    "**Prompt Creation** \n",
    "\n",
    "I will use markdown formatting and newline escape characters (`\\n`) to create a structured prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"Generate a GitHub Pull Request comment describing the code below. \"\n",
    "    \"This should be written in a clear, professional, technical manner so that it can serve as a form of documentation.\\n\\n\"\n",
    "    \n",
    "    \"### Code:\\n\"\n",
    "    \"```python\\n\"\n",
    "    \"from typing import Union\\n\\n\"\n",
    "    \"def safe_divide(a: Union[int, float], b: Union[int, float]) -> Union[float, None]:\\n\"\n",
    "    \"    \\\"\\\"\\\"Safely divides two numbers, returning None if division by zero occurs.\\\"\\\"\\\"\\n\"\n",
    "    \"    try:\\n\"\n",
    "    \"        return a / b\\n\"\n",
    "    \"    except ZeroDivisionError:\\n\"\n",
    "    \"        print(\\\"Error: Division by zero is not allowed.\\\")\\n\"\n",
    "    \"        return None\\n\"\n",
    "    \"```\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating the Pipeline\n",
    "\n",
    "To utilize the Llama model from Hugging Face, we will create a pipeline object that processes the prompt and generates a response.\n",
    "\n",
    "To do this we specify the ID of the LLM model and create a pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Specify the ID of the LLM model\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Create the pipe\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",              # Specify the task type as text generation\n",
    "    model=model_id,                 # Specify the LLM model to be used for generation\n",
    "    torch_dtype=torch.bfloat16,     # Specify the data type for the model weights (bfloat16 for better performance on certain hardware)\n",
    "    device_map=\"auto\"               # Automatically assign model layers to available devices (GPU/CPU) for optimal performance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Prompting the Model\n",
    "\n",
    "With everything set up, we can now prompt the model to generate a response.\n",
    "\n",
    "We will use the pipeline to generate the output, applying certain controls to shape the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a GitHub Pull Request comment describing the code below. This should be written in a clear, professional, technical manner so that it can serve as a form of documentation.\n",
      "\n",
      "### Code:\n",
      "```python\n",
      "from typing import Union\n",
      "\n",
      "def safe_divide(a: Union[int, float], b: Union[int, float]) -> Union[float, None]:\n",
      "    \"\"\"Safely divides two numbers, returning None if division by zero occurs.\"\"\"\n",
      "    try:\n",
      "        return a / b\n",
      "    except ZeroDivisionError:\n",
      "        print(\"Error: Division by zero is not allowed.\")\n",
      "        return None\n",
      "```\n",
      "\n",
      "### Output:\n",
      "```markdown\n",
      "# Safe Division\n",
      "\n",
      "This function safely divides two numbers, returning None if division by zero occurs.\n",
      "\n",
      "## Usage\n",
      "\n",
      "```python\n",
      "safe_divide(10, 2)  # Returns 5.0\n",
      "safe_divide(10, 0)  # Returns None\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate the response using the pipeline\n",
    "output = pipe(\n",
    "    prompt,                        # The input prompt for the model to generate text based on\n",
    "    max_new_tokens=2048,           # Specify the maximum number of new tokens to generate (up to 2048 in this case)\n",
    "    do_sample=False,               # Set to False for deterministic output (the same input will always produce the same output)\n",
    ")\n",
    "\n",
    "# Print the generated text from the output\n",
    "print(output[0][\"generated_text\"])  # Access and print the generated text from the output object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
