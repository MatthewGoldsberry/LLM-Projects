{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation to Grab Pull Request Data from GitHub\n",
    "\n",
    "This notebook demonstrates a method to interact with the GitHub API for fetching and displaying Pull Request (PR) data. It outlines how to set up the environment, make requests, process the responses, and explore ways to integrate this data into an LLM for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environment Setup\n",
    "\n",
    "To run `base_github_pr_inter.ipynb` ensure the `requests` library is install. `requests` is used for making HTTP requests to the GithHub API and can be installed by: \n",
    "\n",
    "```bash\n",
    "pip install requests\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Library\n",
    "\n",
    "The necessary library is imported as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set Up Request URL\n",
    "\n",
    "To fetch PR data, use the GitHub API endpoint:\n",
    "\n",
    "```bash\n",
    "https://api.github.com/repos/{owner}/{repo}/pulls/{pull_number}\n",
    "```\n",
    "\n",
    "Define the `owner`, `repo`, and `pull_number` dynamically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define unique aspects of the URL\n",
    "owner = 'MatthewGoldsberry'\n",
    "repo = 'LLM-Projects'\n",
    "pull_number = 1\n",
    "\n",
    "# Construct the URL\n",
    "url = f'https://api.github.com/repos/{owner}/{repo}/pulls/{pull_number}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Make Request\n",
    "\n",
    "Send a GET request using the `requests` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a request and store response\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on Authentication:**\n",
    "\n",
    "For private repositories or higher request limits, include a GitHub personal access token:\n",
    "\n",
    "```python\n",
    "headers = {\n",
    "    'Authorization': 'YOUR_GITHUB_TOKEN'\n",
    "}\n",
    "```\n",
    "\n",
    "Update the parameters in the `requests.get()` to include `headers`:\n",
    "\n",
    "```python\n",
    "response = requests.get(url, headers=headers)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Process the Response\n",
    "\n",
    "Now that we have the response from the request we can decipher it.\n",
    "\n",
    "The first thing that needs to be done is to check its `status_code` to make sure the request was successful, which is indicated by a value of `200`. \n",
    "\n",
    "Once it is know that the response was successful we will unpack using `json()`.\n",
    "\n",
    "In the following code we will check the `status_code` and if successful unpack via `json` and display the full thing so we can understand what all data is included in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if response.status_code == 200:\n",
    "    # Parse the response JSON\n",
    "    data = response.json()\n",
    "    \n",
    "    # Pretty print the data\n",
    "    for key, value in data.items():\n",
    "        print(f'{key}: {value}')\n",
    "else:\n",
    "    print(f'Error: {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grabbing What We Need**\n",
    "\n",
    "From all of that data there are a couple of items we really want from that which are the `title` and `body` for a general context of the problem and then the `diff_url` so we can see what actually changes were actually made. \n",
    "\n",
    "To isolate these lets run some code to look at what we get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PR\n",
      "This is a test message\n",
      "https://github.com/MatthewGoldsberry/LLM-Projects/pull/1.diff\n"
     ]
    }
   ],
   "source": [
    "# Print the PR title\n",
    "print(data['title'])\n",
    "\n",
    "# Print the PR body\n",
    "print(data['body'])\n",
    "\n",
    "# Print the diff_url\n",
    "print(data['diff_url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since the `diff_url` value is a URL we have to go through a similar process with that as we did the original URL where we make a GET request and then process the data. The major difference between the two though is that the `diff_url` returns plaintext instead of JSON so when processing the response it has to be handled as plaintext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff --git a/GitHub PR AutoGen/base_pretained_model.ipynb b/GitHub PR AutoGen/base_pretained_model.ipynb\n",
      "index a4bdbe5..01e0c28 100644\n",
      "--- a/GitHub PR AutoGen/base_pretained_model.ipynb\t\n",
      "+++ b/GitHub PR AutoGen/base_pretained_model.ipynb\t\n",
      "@@ -15,7 +15,7 @@\n",
      "    \"source\": [\n",
      "     \"### 1. Setting Up the Environment\\n\",\n",
      "     \"\\n\",\n",
      "-    \"Before running `base_pretrained_model.py`, ensure you have the necessary libraries installed. The key libraries used in this implementation are:\\n\",\n",
      "+    \"Before running `base_pretrained_model.ipynb`, ensure you have the necessary libraries installed. The key libraries used in this implementation are:\\n\",\n",
      "     \"* `torch`\\n\",\n",
      "     \"* `transformers`\\n\",\n",
      "     \"* `huggingface_hub`\\n\",\n",
      "@@ -212,7 +212,7 @@\n",
      "     \"# Generate the response using the pipeline\\n\",\n",
      "     \"output = pipe(\\n\",\n",
      "     \"    prompt,                        # The input prompt for the model to generate text based on\\n\",\n",
      "-    \"    max_new_tokens=2048,           # Specify the maximum number of new tokens to generate (up to 2048 in this case)\\n\",\n",
      "+    \"    max_new_tokens=1024,           # Specify the maximum number of new tokens to generate (up to 2048 in this case)\\n\",\n",
      "     \"    do_sample=False,               # Set to False for deterministic output (the same input will always produce the same output)\\n\",\n",
      "     \")\\n\",\n",
      "     \"\\n\",\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch the diff data\n",
    "diff_response = requests.get(data['diff_url'])\n",
    "\n",
    "if diff_response.status_code == 200:\n",
    "    diff_data = diff_response.text\n",
    "    print(diff_data)\n",
    "else:\n",
    "    print(f'Error: {diff_response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. LLM Integration\n",
    "\n",
    "Use the diff output to generate insights with a pre-trained LLM like `Llama 3.2 1B` in this case. It will use a similar prompt to that made in `base_pretained_model.ipynb` with the values obtained from the `diff_url` being injected into it. \n",
    "\n",
    "Below is an excerpt of what the data looks like in markdown when specified as a `diff` type, like it will be in the prompt:\n",
    "\n",
    "```diff\n",
    "@@ -212,7 +212,7 @@\n",
    "     # Generate the response using the pipeline\n",
    "     output = pipe(\n",
    "     prompt,                        # The input prompt for the model to generate text based on\\n\",\n",
    "-    max_new_tokens=2048,           # Specify the maximum number of new tokens to generate (up to 2048 in this case)\n",
    "+    max_new_tokens=1024,           # Specify the maximum number of new tokens to generate (up to 2048 in this case)\n",
    "     do_sample=False,               # Set to False for deterministic output (the same input will always produce the same output)\n",
    "     )\n",
    "```\n",
    "\n",
    "So just to see what we are working with we will inject the diff from the PR into the prompt and prompt the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "C:\\Users\\matth\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\matth\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant tasked with generating a GitHub Pull Request comment. The comment should describe the code changes below in a clear, professional, and technical manner, suitable for documentation purposes.\n",
      "\n",
      "### Code Changes:\n",
      "```diff\n",
      "--- a/GitHub PR AutoGen/base_pretained_model.ipynb+++ b/GitHub PR AutoGen/base_pretained_model.ipynb@@ -15,7 +15,7 @@### 1. Setting Up the Environment\n",
      "\n",
      "-    Before running `base_pretrained_model.py`, ensure you have the necessary libraries installed. The key libraries used in this implementation are:\n",
      "+    Before running `base_pretrained_model.ipynb`, ensure you have the necessary libraries installed. The key libraries used in this implementation are:\n",
      "* `torch`\n",
      "* `transformers`\n",
      "* `huggingface_hub`\n",
      "@@ -212,7 +212,7 @@# Generate the response using the pipeline\n",
      "output = pipe(\n",
      "    prompt,                        # The input prompt for the model to generate text based on\n",
      "-    max_new_tokens=2048,           # Specify the maximum number of new tokens to generate (up to 2048 in this case)\n",
      "+    max_new_tokens=1024,           # Specify the maximum number of new tokens to generate (up to 2048 in this case)\n",
      "    do_sample=False,               # Set to False for deterministic output (the same input will always produce the same output)\n",
      ")\n",
      "```\n",
      "\n",
      "### Code Changes:\n",
      "```diff\n",
      "--- a/GitHub PR AutoGen/base_pretained_model.ipynb+++ b/GitHub PR AutoGen/base_pretained_model.ipynb@@ -15,7 +15,7 @@### 1. Setting Up the Environment\n",
      "\n",
      "-    Before running `base_pretrained_model.py`, ensure you have the necessary libraries installed. The key libraries used in this implementation are:\n",
      "+    Before running `base_pretrained_model.ipynb`, ensure you have the necessary libraries installed. The key libraries used in this implementation are:\n",
      "* `torch`\n",
      "* `transformers`\n",
      "* `huggingface_hub`\n",
      "@@ -212,7 +212,7 @@# Generate the response using the pipeline\n",
      "output = pipe(\n",
      "    prompt,                        # The input prompt for the model to generate text based on\n",
      "-    max_new_tokens=2048,           # Specify the maximum number of new tokens to generate (up to 2048 in this case)\n",
      "+    max_new_tokens=1024,           # Specify the maximum number of new tokens to generate (up to 2048 in this case)\n",
      "    do_sample=False,               # Set to False for deterministic output (the same input will always produce the same output)\n",
      ")\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "from hf_token import llama_3_2_token\n",
    "\n",
    "login(token=llama_3_2_token)\n",
    "\n",
    "# Specify the ID of the LLM model\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Create the pipe\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",              # Specify the task type as text generation\n",
    "    model=model_id,                 # Specify the LLM model to be used for generation\n",
    "    torch_dtype=torch.bfloat16,     # Specify the data type for the model weights (bfloat16 for better performance on certain hardware)\n",
    "    device_map=\"auto\"               # Automatically assign model layers to available devices (GPU/CPU) for optimal performance\n",
    ")\n",
    "\n",
    "# Generate prompt with the whole diff\n",
    "prompt = (\n",
    "    \"You are an AI assistant tasked with generating a GitHub Pull Request comment. \"\n",
    "    \"The comment should describe the code changes below in a clear, professional, and technical manner, suitable for documentation purposes.\\n\\n\"\n",
    "    \n",
    "    \"### Code Changes:\\n\"\n",
    "    \"```diff\\n\"\n",
    "     \"--- a/GitHub PR AutoGen/base_pretained_model.ipynb\"\n",
    "     \"+++ b/GitHub PR AutoGen/base_pretained_model.ipynb\"\t\n",
    "     \"@@ -15,7 +15,7 @@\"\n",
    "     \"### 1. Setting Up the Environment\\n\"\n",
    "     \"\\n\"\n",
    "     \"-    Before running `base_pretrained_model.py`, ensure you have the necessary libraries installed. The key libraries used in this implementation are:\\n\"\n",
    "     \"+    Before running `base_pretrained_model.ipynb`, ensure you have the necessary libraries installed. The key libraries used in this implementation are:\\n\"\n",
    "     \"* `torch`\\n\"\n",
    "     \"* `transformers`\\n\"\n",
    "     \"* `huggingface_hub`\\n\"\n",
    "     \"@@ -212,7 +212,7 @@\"\n",
    "     \"# Generate the response using the pipeline\\n\"\n",
    "     \"output = pipe(\\n\"\n",
    "     \"    prompt,                        # The input prompt for the model to generate text based on\\n\"\n",
    "     \"-    max_new_tokens=2048,           # Specify the maximum number of new tokens to generate (up to 2048 in this case)\\n\"\n",
    "     \"+    max_new_tokens=1024,           # Specify the maximum number of new tokens to generate (up to 2048 in this case)\\n\"\n",
    "     \"    do_sample=False,               # Set to False for deterministic output (the same input will always produce the same output)\\n\"\n",
    "     \")\\n\"   \n",
    "     \"```\\n\\n\"\n",
    ")\n",
    "\n",
    "# Generate the response using the pipeline\n",
    "output = pipe(\n",
    "    prompt,                        # The input prompt for the model to generate text based on\n",
    "    max_new_tokens=2048,           # Specify the maximum number of new tokens to generate (up to 2048 in this case)\n",
    "    do_sample=False,               # Set to False for deterministic output (the same input will always produce the same output)\n",
    ")\n",
    "\n",
    "# Print the generated text from the output\n",
    "print(output[0][\"generated_text\"])  # Access and print the generated text from the output object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Results\n",
    "\n",
    "So now we have explored ways to access any information we may want from the GitHub API but based on the result of the test of plugging that into the model we will have to do some further prompt engineering and work on training the model. As currently, upgrading the model quality is limited by my computer's computational power.\n",
    "\n",
    "**Next Steps**\n",
    "1. Refine prompt engineering fro better LLM-generated outputs.\n",
    "2. Work on training the LLM for better outputs.\n",
    "3. Develop preprocessing steps to summarize large diffs for more concise inputs to the model.\n",
    "\n",
    "**In Summary**\n",
    "\n",
    "This notebook demonstrates the end-to-end process of interacting with the GitHub API, processing PR data, and leveraging LLMs for automated insights (while not perfect, still a POC). It provides a scalable foundation for enhancing development workflows through AI integration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
